{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85efc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#My imports\n",
    "import wrangle as w\n",
    "import env\n",
    "import model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bcbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = w.get_zillow_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d126ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_zillow(df):\n",
    "    ''' This function takes in zillow data, renames columns, replaces whitespace with nan values,\n",
    "    and drops null values. This function returns a df.\n",
    "    '''\n",
    "    # renaming columns\n",
    "    df = df.rename(columns = {'bedroomcnt':'bedrooms', \n",
    "                              'bathroomcnt':'bathrooms', \n",
    "                              'calculatedfinishedsquarefeet':'square_feet',\n",
    "                              'taxvaluedollarcnt':'tax_value', \n",
    "                              'yearbuilt':'year_built',\n",
    "                              'taxamount':'tax_amount',\n",
    "                              'lotsizesquarefeet' : 'lot_size'\n",
    "                              'transactiondate' : 'transaction_date'\n",
    "                              'parcelid' : 'parcel_id'\n",
    "                    })\n",
    "    # Replace a whitespace sequence or empty with a NaN value and reassign this manipulation to df. \n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    # Removes null values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Converting some columns from float to integers or objects\n",
    "    df[\"fips\"] = df[\"fips\"].astype(int)\n",
    "    df[\"year_built\"] = df[\"year_built\"].astype(int)\n",
    "    df[\"bedrooms\"] = df[\"bedrooms\"].astype(int)    \n",
    "    df[\"square_feet\"] = df[\"square_feet\"].astype(int)\n",
    "    df[\"tax_value\"] = df[\"tax_value\"].astype(int)\n",
    "    \n",
    "    # Relabeling FIPS data\n",
    "    df['county'] = df.fips.replace({6037:'Los Angeles',\n",
    "                       6059:'Orange',\n",
    "                       6111:'Ventura'})\n",
    "    \n",
    "    #Creating new column for home age using year_built, casting as integer\n",
    "    df['home_age'] = 2017- df.year_built\n",
    "    df[\"home_age\"] = df[\"home_age\"].astype(int)\n",
    "    \n",
    "    #Remove outliers\n",
    "    df = remove_outliers(df,['bedrooms','bathrooms','square_feet','tax_value','tax_amount'])\n",
    "    \n",
    "    #Make dummy variables from county\n",
    "    df = pd.get_dummies(df, columns=['county'], drop_first=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the zillow data\n",
    "df = w.clean_zillow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88520e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "train, validate, test = m.train_validate_test_split(df, 'tax_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Train length is {len(train)} rows')\n",
    "print(f' Validate length is {len(validate)} rows')\n",
    "print(f' Test length is {len(test)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d960d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data before further splitting\n",
    "train, validate, test = m.scale_data(train, validate, test, columns_to_scale=['bedrooms', 'bathrooms', 'square_feet','home_age'], return_scaler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8126cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bdc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for model prep\n",
    "def model_prep(train,validate,test):\n",
    "    '''\n",
    "    This function prepares train, validate, test for modeling by dropping columns not necessary\n",
    "    or compatible with modeling algorithms.\n",
    "    '''\n",
    "    # drop columns not needed\n",
    "    keep_cols = ['bedrooms',\n",
    "                 'bathrooms',\n",
    "                 'square_feet',\n",
    "                 'tax_value',\n",
    "                 'home_age',\n",
    "                 'county_Los Angeles',\n",
    "                 'county_Orange',\n",
    "                 'county_Ventura']\n",
    "    \n",
    "    train = train[keep_cols]\n",
    "    validate = validate[keep_cols]\n",
    "    test = test[keep_cols]\n",
    "\n",
    "    # Split data into predicting variables (X) and target variable (y) and reset the index for each dataframe\n",
    "    X_train = train.drop(columns='tax_value').reset_index(drop=True)\n",
    "    y_train = train[['tax_value']].reset_index(drop=True)\n",
    "\n",
    "    X_validate = validate.drop(columns='tax_value').reset_index(drop=True)\n",
    "    y_validate = validate[['tax_value']].reset_index(drop=True)\n",
    "\n",
    "    X_test = test.drop(columns='tax_value').reset_index(drop=True)\n",
    "    y_test = test[['tax_value']].reset_index(drop=True)\n",
    "    \n",
    "    return X_train, X_validate, X_test, y_train, y_validate, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f74d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, X_test, y_train, y_validate, y_test = m.model_prep(train,validate,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Target Variable\n",
    "plt.hist(y_train)\n",
    "plt.xlabel('Tax Value')\n",
    "plt.ylabel('Count of Homes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dde0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put y_train and y_validate into dataframes to append the new columns with predicted values. \n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict tax_value_mean\n",
    "tax_value_pred_mean = y_train['tax_value'].mean()\n",
    "y_train['tax_value_pred_mean'] = tax_value_pred_mean\n",
    "y_validate['tax_value_pred_mean'] = tax_value_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d273f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. compute tax_value_median\n",
    "tax_value_pred_median = y_train['tax_value'].median()\n",
    "y_train['tax_value_pred_median'] = tax_value_pred_median\n",
    "y_validate['tax_value_pred_median'] = tax_value_pred_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RMSE of tax_value_mean\n",
    "rmse_train = mean_squared_error(y_train.tax_value, y_train.tax_value_pred_mean)**(1/2)\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_pred_mean)**(1/2)\n",
    "\n",
    "print(\"RMSE using Mean\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RMSE of tax_value_median\n",
    "rmse_train = mean_squared_error(y_train.tax_value, y_train.tax_value_pred_median)**(1/2)\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_pred_median)**(1/2)\n",
    "\n",
    "print(\"RMSE using Median\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a689145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.hist(y_train.tax_value, color='blue', alpha=.5, label=\"Actual Tax Value\")\n",
    "plt.hist(y_train.tax_value_pred_mean, bins=1, color='red', alpha=.5, rwidth=100, label=\"Predicted Tax Value - Mean\")\n",
    "plt.hist(y_train.tax_value_pred_median, bins=1, color='orange', alpha=.5, rwidth=100, label=\"Predicted Tax Value - Median\")\n",
    "plt.xlabel(\"Tax Value\")\n",
    "plt.ylabel(\"Number of Homes\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86727ae",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ec41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to train. Specifying the column in y_train, since we have converted it to a dataframe from a series\n",
    "lm.fit(X_train, y_train.tax_value)\n",
    "\n",
    "# predict train\n",
    "y_train['tax_value_pred_lm'] = lm.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.tax_value, y_train.tax_value_pred_lm)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['tax_value_pred_lm'] = lm.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_pred_lm)**(1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235c23a",
   "metadata": {},
   "source": [
    "#### So far, Linear Regression looks to reduce RMSE in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da204054",
   "metadata": {},
   "source": [
    "### Lasso-Lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=1.0)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lars.fit(X_train, y_train.tax_value)\n",
    "\n",
    "# predict train\n",
    "y_train['tax_value_pred_lars'] = lars.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.tax_value, y_train.tax_value_pred_lars)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['tax_value_pred_lars'] = lars.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_pred_lars)**(1/2)\n",
    "\n",
    "print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77609177",
   "metadata": {},
   "source": [
    "### Lasso Lars looks to perform about the same for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065773d",
   "metadata": {},
   "source": [
    "### Generalized Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375828be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "glm = TweedieRegressor(power=0, alpha=0)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "glm.fit(X_train, y_train.tax_value)\n",
    "\n",
    "# predict train\n",
    "y_train['tax_value_pred_glm'] = glm.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.tax_value, y_train.tax_value_pred_glm)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['tax_value_pred_glm'] = glm.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_pred_glm)**(1/2)\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=0 & alpha=0\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c49e5",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a54b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "X_train_degree2 = pf.fit_transform(X_train)\n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n",
    "X_validate_degree2 = pf.transform(X_validate)\n",
    "X_test_degree2 = pf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm2 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lm2.fit(X_train_degree2, y_train.tax_value)\n",
    "\n",
    "# predict train\n",
    "y_train['tax_value_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.tax_value, y_train.tax_value_pred_lm2)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['tax_value_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_pred_lm2)**(1/2)\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bea39",
   "metadata": {},
   "source": [
    "#### Polynomial features performed the best, so far, without a pointed attempt to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d05aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Actual vs predicted values\n",
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_validate.tax_value, y_validate.tax_value_pred_mean, alpha=.5, color=\"red\", label='_nolegend_')\n",
    "plt.annotate(\"Baseline: Predict Using Mean\", (16, 9.5))\n",
    "plt.plot(y_validate.tax_value, y_validate.tax_value, alpha=.5, color=\"blue\", label='_nolegend_')\n",
    "plt.annotate(\"The Ideal Line: Predicted = Actual\", (.5, 3.5), rotation=15.5)\n",
    "\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_lm, \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_glm, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_lm2, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Tax Value\")\n",
    "plt.ylabel(\"Predicted Tax Value\")\n",
    "plt.title(\"Where are predictions more extreme? More modest?\")\n",
    "# plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "# plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Plots: Plotting the Errors in Predictions\n",
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axhline(label=\"No Error\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_lm-y_validate.tax_value, \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_glm-y_validate.tax_value, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_lm2-y_validate.tax_value, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Tax Value\")\n",
    "plt.ylabel(\"Residual/Error: Predicted Tax Value - Actual Tax Value\")\n",
    "plt.title(\"Do the size of errors change as the actual value changes?\")\n",
    "#plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "#plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e037ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4174d10",
   "metadata": {},
   "source": [
    "### Let's Pivot to examine RFE to improve modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple linear model evaluation\n",
    "lm.coef_, lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7ab98",
   "metadata": {},
   "source": [
    "### Using select K Best to evaluate my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81080b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: f_regression stats test, give me 3 features\n",
    "f_selector = SelectKBest(f_regression, k=3)\n",
    "# find the top 3 X's correlated with y\n",
    "f_selector.fit(X_train, y_train)\n",
    "# boolean mask of whether the column was selected or not. \n",
    "feature_mask = f_selector.get_support()\n",
    "# get list of top K features. \n",
    "f_feature = X_train.iloc[:,feature_mask].columns.tolist()\n",
    "f_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53ee04",
   "metadata": {},
   "source": [
    "### Using RFE to evaluate my features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ML algorithm\n",
    "lm = LinearRegression()\n",
    "\n",
    "# create the rfe object, indicating the ML object (lm) and the number of features I want to end up with. \n",
    "rfe = RFE(lm, n_features_to_select=3)\n",
    "\n",
    "# fit the data using RFE\n",
    "rfe.fit(X_train,y_train)  \n",
    "\n",
    "# get the mask of the columns selected\n",
    "feature_mask = rfe.support_\n",
    "\n",
    "# get list of the column names. \n",
    "rfe_feature = X_train.iloc[:,feature_mask].columns.tolist()\n",
    "rfe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ce833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view list of columns and their ranking\n",
    "\n",
    "# get the ranks\n",
    "var_ranks = rfe.ranking_\n",
    "# get the variable names\n",
    "var_names = X_train.columns.tolist()\n",
    "# combine ranks and names into a df for clean viewing\n",
    "rfe_ranks_df = pd.DataFrame({'Var': var_names, 'Rank': var_ranks})\n",
    "# sort the df by rank\n",
    "rfe_ranks_df.sort_values('Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468db6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
